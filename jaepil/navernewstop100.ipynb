{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 종목에 대한 Naver 뉴스 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd()\n",
    "WORKSPACE_PATH = CWD.parent\n",
    "COMMON_PATH = WORKSPACE_PATH / 'common'\n",
    "DATA_PATH = WORKSPACE_PATH / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기 (Top 100 returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df = pd.read_pickle(DATA_PATH / 'returns_df_top100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020.01.02', '2024.07.12')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE = returns_df.index[0].strftime('%Y.%m.%d')\n",
    "END_DATE = returns_df.index[-1].strftime('%Y.%m.%d')\n",
    "\n",
    "START_DATE, END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_tickers = returns_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(DATA_PATH / 'top100_tickers.pkl', 'wb') as f:\n",
    "#     pickle.dump(top100_tickers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 종목뉴스 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 제목, 링크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(sid, page):\n",
    "    sid = str(sid).zfill(6)\n",
    "    url = f'https://finance.naver.com/item/news_news.naver?code={sid}&page={page}'\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    news_list = []\n",
    "    table = soup.find('table', {'class': 'type5'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        # Identify main article rows\n",
    "        if row.get('class') is None or 'relation_tit' in row.get('class', []):\n",
    "            cols = row.find_all('td')\n",
    "            if cols and cols[0].find('a', {'class': 'tit'}):\n",
    "                title_tag = cols[0].find('a', {'class': 'tit'})\n",
    "                if title_tag:\n",
    "                    title = title_tag.text.strip()\n",
    "                    href = title_tag['href']\n",
    "                    if 'news_read' in href:\n",
    "                        params = href.split('?')[1]\n",
    "                        params_dict = dict(param.split('=') for param in params.split('&'))\n",
    "                        office_id = params_dict.get('office_id')\n",
    "                        article_id = params_dict.get('article_id')\n",
    "                        full_link = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'\n",
    "                        info_provider = cols[1].text.strip()\n",
    "                        date = cols[2].text.strip()\n",
    "                        news_list.append([sid, title, date, info_provider, full_link])\n",
    "        # Identify child article rows\n",
    "        elif 'relation_lst' in row.get('class', []):\n",
    "            sub_table = row.find('table', {'class': 'type5'})\n",
    "            sub_rows = sub_table.find_all('tr')\n",
    "            for sub_row in sub_rows:\n",
    "                sub_cols = sub_row.find_all('td')\n",
    "                if sub_cols and sub_cols[0].find('a', {'class': 'tit'}):\n",
    "                    title_tag = sub_cols[0].find('a', {'class': 'tit'})\n",
    "                    if title_tag:\n",
    "                        title = title_tag.text.strip()\n",
    "                        href = title_tag['href']\n",
    "                        if 'news_read' in href:\n",
    "                            params = href.split('?')[1]\n",
    "                            params_dict = dict(param.split('=') for param in params.split('&'))\n",
    "                            office_id = params_dict.get('office_id')\n",
    "                            article_id = params_dict.get('article_id')\n",
    "                            full_link = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'\n",
    "                            info_provider = sub_cols[1].text.strip()\n",
    "                            date = sub_cols[2].text.strip()\n",
    "                            news_list.append([sid, title, date, info_provider, full_link])\n",
    "    return news_list\n",
    "\n",
    "# Function to scrape multiple pages until a specific date\n",
    "def scrape_until_date(sid, end_date):\n",
    "    page = 1\n",
    "    all_news = []\n",
    "    last_page_content = None\n",
    "    \n",
    "    while True:\n",
    "        news_list = scrape_page(sid, page)\n",
    "        if not news_list:\n",
    "            break\n",
    "        \n",
    "        current_page_content = str(news_list)\n",
    "        if current_page_content == last_page_content:\n",
    "            break\n",
    "\n",
    "        for news in news_list:\n",
    "            news_date = news[2]\n",
    "            if news_date < end_date:\n",
    "                return all_news\n",
    "            \n",
    "            all_news.append(news)\n",
    "        \n",
    "        last_page_content = current_page_content\n",
    "        page += 1\n",
    "\n",
    "    return all_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sid = '007110'  # Example stock code\n",
    "end_date = '2024.07.14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020.01.02'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping #0: 000060...\n",
      "Time taken for scraping #0: 6.68 seconds\n",
      "Scraping #1: 000080...\n",
      "Time taken for scraping #1: 55.85 seconds\n",
      "Scraping #2: 000100...\n",
      "Time taken for scraping #2: 37.04 seconds\n",
      "Scraping #3: 000120...\n",
      "Time taken for scraping #3: 51.01 seconds\n",
      "Scraping #4: 000150...\n",
      "Time taken for scraping #4: 33.23 seconds\n",
      "Scraping #5: 000210...\n",
      "Time taken for scraping #5: 4.74 seconds\n",
      "Scraping #6: 000250...\n",
      "Time taken for scraping #6: 15.00 seconds\n",
      "Scraping #7: 000270...\n",
      "Time taken for scraping #7: 62.99 seconds\n",
      "Scraping #8: 000660...\n",
      "Time taken for scraping #8: 66.12 seconds\n",
      "Scraping #9: 000720...\n",
      "Time taken for scraping #9: 58.14 seconds\n",
      "Scraping #10: 000810...\n",
      "Time taken for scraping #10: 52.75 seconds\n",
      "Scraping #11: 000990...\n",
      "Time taken for scraping #11: 10.11 seconds\n",
      "Scraping #12: 001040...\n",
      "Time taken for scraping #12: 51.20 seconds\n",
      "Scraping #13: 001440...\n",
      "Time taken for scraping #13: 17.15 seconds\n",
      "Scraping #14: 001450...\n",
      "Time taken for scraping #14: 51.80 seconds\n",
      "Scraping #15: 001570...\n",
      "Time taken for scraping #15: 10.65 seconds\n",
      "Scraping #16: 002380...\n",
      "Time taken for scraping #16: 11.78 seconds\n",
      "Scraping #17: 002790...\n",
      "Time taken for scraping #17: 54.13 seconds\n",
      "Scraping #18: 003000...\n",
      "Time taken for scraping #18: 6.00 seconds\n",
      "Scraping #19: 003090...\n",
      "Time taken for scraping #19: 0.52 seconds\n",
      "Scraping #20: 003230...\n",
      "Time taken for scraping #20: 40.13 seconds\n",
      "Scraping #21: 003410...\n",
      "Time taken for scraping #21: 0.16 seconds\n",
      "Scraping #22: 003490...\n",
      "Time taken for scraping #22: 59.85 seconds\n",
      "Scraping #23: 003550...\n",
      "Time taken for scraping #23: 61.69 seconds\n",
      "Scraping #24: 003670...\n",
      "Time taken for scraping #24: 56.64 seconds\n",
      "Scraping #25: 004020...\n",
      "Time taken for scraping #25: 45.76 seconds\n",
      "Scraping #26: 004170...\n",
      "Time taken for scraping #26: 66.06 seconds\n",
      "Scraping #27: 004990...\n",
      "Time taken for scraping #27: 29.06 seconds\n",
      "Scraping #28: 005070...\n",
      "Time taken for scraping #28: 5.73 seconds\n",
      "Scraping #29: 005380...\n",
      "Time taken for scraping #29: 73.38 seconds\n",
      "Scraping #30: 005420...\n",
      "Time taken for scraping #30: 2.30 seconds\n",
      "Scraping #31: 005490...\n",
      "Time taken for scraping #31: 63.95 seconds\n",
      "Scraping #32: 005830...\n",
      "Time taken for scraping #32: 45.93 seconds\n",
      "Scraping #33: 005930...\n",
      "Time taken for scraping #33: 76.00 seconds\n",
      "Scraping #34: 005940...\n",
      "Time taken for scraping #34: 58.36 seconds\n",
      "Scraping #35: 006260...\n",
      "Time taken for scraping #35: 29.19 seconds\n",
      "Scraping #36: 006280...\n",
      "Time taken for scraping #36: 23.22 seconds\n",
      "Scraping #37: 006360...\n",
      "Time taken for scraping #37: 62.94 seconds\n",
      "Scraping #38: 006400...\n",
      "Time taken for scraping #38: 63.12 seconds\n",
      "Scraping #39: 006800...\n",
      "Time taken for scraping #39: 57.90 seconds\n",
      "Scraping #40: 007070...\n",
      "Time taken for scraping #40: 57.48 seconds\n",
      "Scraping #41: 008560...\n",
      "Time taken for scraping #41: 28.78 seconds\n",
      "Scraping #42: 008770...\n",
      "Time taken for scraping #42: 64.10 seconds\n",
      "Scraping #43: 008930...\n",
      "Time taken for scraping #43: 44.19 seconds\n",
      "Scraping #44: 009150...\n",
      "Time taken for scraping #44: 57.25 seconds\n",
      "Scraping #45: 009240...\n",
      "Time taken for scraping #45: 13.26 seconds\n",
      "Scraping #46: 009540...\n",
      "Time taken for scraping #46: 58.40 seconds\n",
      "Scraping #47: 009830...\n",
      "Time taken for scraping #47: 52.06 seconds\n",
      "Scraping #48: 010060...\n",
      "Time taken for scraping #48: 29.58 seconds\n",
      "Scraping #49: 010120...\n",
      "Time taken for scraping #49: 48.00 seconds\n",
      "Scraping #50: 010130...\n",
      "Time taken for scraping #50: 39.39 seconds\n",
      "Scraping #51: 010140...\n",
      "Time taken for scraping #51: 53.17 seconds\n",
      "Scraping #52: 010620...\n",
      "Time taken for scraping #52: 24.44 seconds\n",
      "Scraping #53: 010950...\n",
      "Time taken for scraping #53: 47.74 seconds\n",
      "Scraping #54: 011070...\n",
      "Time taken for scraping #54: 60.35 seconds\n",
      "Scraping #55: 011170...\n",
      "Time taken for scraping #55: 80.20 seconds\n",
      "Scraping #56: 011200...\n",
      "Time taken for scraping #56: 26.29 seconds\n",
      "Scraping #57: 011780...\n",
      "Time taken for scraping #57: 26.27 seconds\n",
      "Scraping #58: 011790...\n",
      "Time taken for scraping #58: 16.24 seconds\n",
      "Scraping #59: 012330...\n",
      "Time taken for scraping #59: 58.48 seconds\n",
      "Scraping #60: 012450...\n",
      "Time taken for scraping #60: 57.87 seconds\n",
      "Scraping #61: 012510...\n",
      "Time taken for scraping #61: 12.76 seconds\n",
      "Scraping #62: 012750...\n",
      "Time taken for scraping #62: 8.41 seconds\n",
      "Scraping #63: 014680...\n",
      "Time taken for scraping #63: 2.60 seconds\n",
      "Scraping #64: 015760...\n",
      "Time taken for scraping #64: 66.95 seconds\n",
      "Scraping #65: 016360...\n",
      "Time taken for scraping #65: 64.02 seconds\n",
      "Scraping #66: 017670...\n",
      "Time taken for scraping #66: 76.82 seconds\n",
      "Scraping #67: 018260...\n",
      "Time taken for scraping #67: 61.18 seconds\n",
      "Scraping #68: 018880...\n",
      "Time taken for scraping #68: 7.41 seconds\n",
      "Scraping #69: 019170...\n",
      "Time taken for scraping #69: 2.86 seconds\n",
      "Scraping #70: 020150...\n",
      "Time taken for scraping #70: 17.25 seconds\n",
      "Scraping #71: 021240...\n",
      "Time taken for scraping #71: 23.92 seconds\n",
      "Scraping #72: 022100...\n",
      "Time taken for scraping #72: 58.13 seconds\n",
      "Scraping #73: 023530...\n",
      "Time taken for scraping #73: 52.01 seconds\n",
      "Scraping #74: 024110...\n",
      "Time taken for scraping #74: 62.67 seconds\n",
      "Scraping #75: 026960...\n",
      "Time taken for scraping #75: 5.49 seconds\n",
      "Scraping #76: 028050...\n",
      "Time taken for scraping #76: 22.76 seconds\n",
      "Scraping #77: 028260...\n",
      "Time taken for scraping #77: 62.91 seconds\n",
      "Scraping #78: 028300...\n",
      "Time taken for scraping #78: 38.99 seconds\n",
      "Scraping #79: 028670...\n",
      "Time taken for scraping #79: 23.64 seconds\n",
      "Scraping #80: 029780...\n",
      "Time taken for scraping #80: 36.51 seconds\n",
      "Scraping #81: 030200...\n",
      "Time taken for scraping #81: 73.71 seconds\n",
      "Scraping #82: 032500...\n",
      "Time taken for scraping #82: 0.58 seconds\n",
      "Scraping #83: 032640...\n",
      "Time taken for scraping #83: 70.87 seconds\n",
      "Scraping #84: 032830...\n",
      "Time taken for scraping #84: 59.66 seconds\n",
      "Scraping #85: 033780...\n",
      "Time taken for scraping #85: 43.97 seconds\n",
      "Scraping #86: 034020...\n",
      "Time taken for scraping #86: 63.45 seconds\n",
      "Scraping #87: 034220...\n",
      "Time taken for scraping #87: 64.98 seconds\n",
      "Scraping #88: 034730...\n",
      "Time taken for scraping #88: 76.67 seconds\n",
      "Scraping #89: 035250...\n",
      "Time taken for scraping #89: 15.42 seconds\n",
      "Scraping #90: 035420...\n",
      "Time taken for scraping #90: 77.01 seconds\n",
      "Scraping #91: 035720...\n",
      "Time taken for scraping #91: 73.68 seconds\n",
      "Scraping #92: 035760...\n",
      "Time taken for scraping #92: 34.48 seconds\n",
      "Scraping #93: 035900...\n",
      "Time taken for scraping #93: 52.15 seconds\n",
      "Scraping #94: 036460...\n",
      "Time taken for scraping #94: 53.11 seconds\n",
      "Scraping #95: 036490...\n",
      "Time taken for scraping #95: 0.08 seconds\n",
      "Scraping #96: 036570...\n",
      "Time taken for scraping #96: 80.36 seconds\n",
      "Scraping #97: 039490...\n",
      "Time taken for scraping #97: 59.31 seconds\n",
      "Scraping #98: 041510...\n",
      "Time taken for scraping #98: 58.40 seconds\n",
      "Scraping #99: 041960...\n",
      "Time taken for scraping #99: 0.54 seconds\n",
      "Scraping #100: 042660...\n",
      "Time taken for scraping #100: 64.65 seconds\n",
      "Scraping #101: 042670...\n",
      "Time taken for scraping #101: 22.88 seconds\n",
      "Scraping #102: 042700...\n",
      "Time taken for scraping #102: 44.42 seconds\n",
      "Scraping #103: 047040...\n",
      "Time taken for scraping #103: 59.58 seconds\n",
      "Scraping #104: 047050...\n",
      "Time taken for scraping #104: 57.75 seconds\n",
      "Scraping #105: 047810...\n",
      "Time taken for scraping #105: 52.73 seconds\n",
      "Scraping #106: 051900...\n",
      "Time taken for scraping #106: 60.26 seconds\n",
      "Scraping #107: 051910...\n",
      "Time taken for scraping #107: 60.98 seconds\n",
      "Scraping #108: 052690...\n",
      "Time taken for scraping #108: 10.42 seconds\n",
      "Scraping #109: 055550...\n",
      "Time taken for scraping #109: 70.21 seconds\n",
      "Scraping #110: 058470...\n",
      "Time taken for scraping #110: 40.72 seconds\n",
      "Scraping #111: 064350...\n",
      "Time taken for scraping #111: 40.05 seconds\n",
      "Scraping #112: 065350...\n",
      "Time taken for scraping #112: 21.56 seconds\n",
      "Scraping #113: 066570...\n",
      "Time taken for scraping #113: 76.74 seconds\n",
      "Scraping #114: 066970...\n",
      "Time taken for scraping #114: 36.87 seconds\n",
      "Scraping #115: 068270...\n",
      "Time taken for scraping #115: 57.99 seconds\n",
      "Scraping #116: 068760...\n",
      "Time taken for scraping #116: 53.39 seconds\n",
      "Scraping #117: 069620...\n",
      "Time taken for scraping #117: 37.36 seconds\n",
      "Scraping #118: 071050...\n",
      "Time taken for scraping #118: 55.34 seconds\n",
      "Scraping #119: 078930...\n",
      "Time taken for scraping #119: 39.01 seconds\n",
      "Scraping #120: 079440...\n",
      "Time taken for scraping #120: 0.06 seconds\n",
      "Scraping #121: 079550...\n",
      "Time taken for scraping #121: 42.80 seconds\n",
      "Scraping #122: 081660...\n",
      "Time taken for scraping #122: 7.91 seconds\n",
      "Scraping #123: 084990...\n",
      "Time taken for scraping #123: 2.58 seconds\n",
      "Scraping #124: 086280...\n",
      "Time taken for scraping #124: 23.58 seconds\n",
      "Scraping #125: 086520...\n",
      "Time taken for scraping #125: 58.48 seconds\n",
      "Scraping #126: 086790...\n",
      "Time taken for scraping #126: 67.50 seconds\n",
      "Scraping #127: 086900...\n",
      "Time taken for scraping #127: 13.63 seconds\n",
      "Scraping #128: 088350...\n",
      "Time taken for scraping #128: 59.37 seconds\n",
      "Scraping #129: 090430...\n",
      "Time taken for scraping #129: 56.66 seconds\n",
      "Scraping #130: 091990...\n",
      "Time taken for scraping #130: 52.61 seconds\n",
      "Scraping #131: 095700...\n",
      "Time taken for scraping #131: 2.62 seconds\n",
      "Scraping #132: 096530...\n",
      "Time taken for scraping #132: 3.39 seconds\n",
      "Scraping #133: 096770...\n",
      "Time taken for scraping #133: 64.35 seconds\n",
      "Scraping #134: 097950...\n",
      "Time taken for scraping #134: 63.03 seconds\n",
      "Scraping #135: 105560...\n",
      "Time taken for scraping #135: 62.56 seconds\n",
      "Scraping #136: 112040...\n",
      "Time taken for scraping #136: 77.64 seconds\n",
      "Scraping #137: 112610...\n",
      "Time taken for scraping #137: 5.58 seconds\n",
      "Scraping #138: 128940...\n",
      "Time taken for scraping #138: 60.54 seconds\n",
      "Scraping #139: 137310...\n",
      "Time taken for scraping #139: 4.30 seconds\n",
      "Scraping #140: 138040...\n",
      "Time taken for scraping #140: 20.72 seconds\n",
      "Scraping #141: 139480...\n",
      "Time taken for scraping #141: 67.18 seconds\n",
      "Scraping #142: 145020...\n",
      "Time taken for scraping #142: 10.93 seconds\n",
      "Scraping #143: 161390...\n",
      "Time taken for scraping #143: 48.05 seconds\n",
      "Scraping #144: 180640...\n",
      "Time taken for scraping #144: 9.62 seconds\n",
      "Scraping #145: 196170...\n",
      "Time taken for scraping #145: 51.63 seconds\n",
      "Scraping #146: 204320...\n",
      "Time taken for scraping #146: 9.29 seconds\n",
      "Scraping #147: 207940...\n",
      "Time taken for scraping #147: 58.77 seconds\n",
      "Scraping #148: 235980...\n",
      "Time taken for scraping #148: 2.96 seconds\n",
      "Scraping #149: 241560...\n",
      "Time taken for scraping #149: 18.91 seconds\n",
      "Scraping #150: 247540...\n",
      "Time taken for scraping #150: 54.91 seconds\n",
      "Scraping #151: 251270...\n",
      "Time taken for scraping #151: 58.39 seconds\n",
      "Scraping #152: 253450...\n",
      "Time taken for scraping #152: 9.78 seconds\n",
      "Scraping #153: 259960...\n",
      "Time taken for scraping #153: 58.45 seconds\n",
      "Scraping #154: 263750...\n",
      "Time taken for scraping #154: 43.62 seconds\n",
      "Scraping #155: 267250...\n",
      "Time taken for scraping #155: 62.38 seconds\n",
      "Scraping #156: 267260...\n",
      "Time taken for scraping #156: 26.47 seconds\n",
      "Scraping #157: 268600...\n",
      "Time taken for scraping #157: 2.00 seconds\n",
      "Scraping #158: 271560...\n",
      "Time taken for scraping #158: 22.23 seconds\n",
      "Scraping #159: 272210...\n",
      "Time taken for scraping #159: 53.48 seconds\n",
      "Scraping #160: 277810...\n",
      "Time taken for scraping #160: 52.27 seconds\n",
      "Scraping #161: 278280...\n",
      "Time taken for scraping #161: 0.83 seconds\n",
      "Scraping #162: 282330...\n",
      "Time taken for scraping #162: 53.18 seconds\n",
      "Scraping #163: 285130...\n",
      "Time taken for scraping #163: 2.80 seconds\n",
      "Scraping #164: 293490...\n",
      "Time taken for scraping #164: 59.83 seconds\n",
      "Scraping #165: 298020...\n",
      "Time taken for scraping #165: 3.03 seconds\n",
      "Scraping #166: 298040...\n",
      "Time taken for scraping #166: 5.27 seconds\n",
      "Scraping #167: 298050...\n",
      "Time taken for scraping #167: 2.85 seconds\n",
      "Scraping #168: 302440...\n",
      "Time taken for scraping #168: 26.56 seconds\n",
      "Scraping #169: 307950...\n",
      "Time taken for scraping #169: 19.30 seconds\n",
      "Scraping #170: 316140...\n",
      "Time taken for scraping #170: 61.43 seconds\n",
      "Scraping #171: 323410...\n",
      "Time taken for scraping #171: 61.66 seconds\n",
      "Scraping #172: 323990...\n",
      "Time taken for scraping #172: 2.90 seconds\n",
      "Scraping #173: 326030...\n",
      "Time taken for scraping #173: 25.12 seconds\n",
      "Scraping #174: 328130...\n",
      "Time taken for scraping #174: 16.19 seconds\n",
      "Scraping #175: 329180...\n",
      "Time taken for scraping #175: 63.96 seconds\n",
      "Scraping #176: 336260...\n",
      "Time taken for scraping #176: 2.98 seconds\n",
      "Scraping #177: 348370...\n",
      "Time taken for scraping #177: 14.16 seconds\n",
      "Scraping #178: 352820...\n",
      "Time taken for scraping #178: 45.42 seconds\n",
      "Scraping #179: 361610...\n",
      "Time taken for scraping #179: 15.71 seconds\n",
      "Scraping #180: 373220...\n",
      "Time taken for scraping #180: 66.25 seconds\n",
      "Scraping #181: 377300...\n",
      "Time taken for scraping #181: 65.65 seconds\n",
      "Scraping #182: 383220...\n",
      "Time taken for scraping #182: 1.50 seconds\n",
      "Scraping #183: 402340...\n",
      "Time taken for scraping #183: 32.70 seconds\n",
      "Scraping #184: 403870...\n",
      "Time taken for scraping #184: 17.29 seconds\n",
      "Scraping #185: 417200...\n",
      "Time taken for scraping #185: 17.05 seconds\n",
      "Scraping #186: 443060...\n",
      "Time taken for scraping #186: 25.76 seconds\n",
      "Scraping #187: 450080...\n",
      "Time taken for scraping #187: 41.72 seconds\n",
      "Scraping #188: 454910...\n",
      "Time taken for scraping #188: 54.88 seconds\n",
      "Scraping #189: 462870...\n",
      "Time taken for scraping #189: 4.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "all_news = []\n",
    "\n",
    "for i, sid in enumerate(top100_tickers):\n",
    "    sid = sid[1:]\n",
    "    print(f'Scraping #{i}: {sid}...')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    all_news += scrape_until_date(sid, START_DATE)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_taken = end_time - start_time\n",
    "    print(f'Time taken for scraping #{i}: {time_taken:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>info_provider</th>\n",
       "      <th>full_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000060</td>\n",
       "      <td>'여의도 금융중심 계획' 결정고시 눈앞…시행사들 기다림 끝나간다</td>\n",
       "      <td>2024.07.18 21:03</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/018/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000060</td>\n",
       "      <td>힘내요, 한 발 한 발…든든한 금융지주가 사다리를 놓아줍니다</td>\n",
       "      <td>2024.07.18 16:26</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000060</td>\n",
       "      <td>\"휴대폰 파손·항공 지연 대비\"…네이버페이 여행보험 플랜 비교</td>\n",
       "      <td>2024.07.18 10:16</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000060</td>\n",
       "      <td>보험사 2분기 실적, 생보 웃고 손보 운다…제3보험·車보험 변수</td>\n",
       "      <td>2024.07.17 08:44</td>\n",
       "      <td>아시아경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/277/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000060</td>\n",
       "      <td>진격의 삼성·키움증권, 순익 2·3위로… 메리츠·NH 넘어서나?</td>\n",
       "      <td>2024.07.16 16:35</td>\n",
       "      <td>머니S</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/417/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237297</th>\n",
       "      <td>462870</td>\n",
       "      <td>IPO 앞둔 시프트업, `희망퇴직·성추문 의혹` 잇단 악재</td>\n",
       "      <td>2023.07.24 16:55</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/029/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237298</th>\n",
       "      <td>462870</td>\n",
       "      <td>[단독] '성추문·폭언 의혹' 시프트업 투자사 대표 \"책임지고 퇴사하겠...</td>\n",
       "      <td>2023.07.24 12:53</td>\n",
       "      <td>아이뉴스24</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/031/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237299</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니차일드', 9월 21일 서비스 종료</td>\n",
       "      <td>2023.07.20 17:07</td>\n",
       "      <td>지디넷코리아</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/092/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237300</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니 차일드', 9월 21일 서비스 종료</td>\n",
       "      <td>2023.07.20 16:47</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/030/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237301</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니 차일드', 9월21일 서비스 종료</td>\n",
       "      <td>2023.07.20 16:22</td>\n",
       "      <td>데일리e스포츠</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/347/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1237302 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid                                       title              date  \\\n",
       "0        000060         '여의도 금융중심 계획' 결정고시 눈앞…시행사들 기다림 끝나간다  2024.07.18 21:03   \n",
       "1        000060           힘내요, 한 발 한 발…든든한 금융지주가 사다리를 놓아줍니다  2024.07.18 16:26   \n",
       "2        000060          \"휴대폰 파손·항공 지연 대비\"…네이버페이 여행보험 플랜 비교  2024.07.18 10:16   \n",
       "3        000060         보험사 2분기 실적, 생보 웃고 손보 운다…제3보험·車보험 변수  2024.07.17 08:44   \n",
       "4        000060         진격의 삼성·키움증권, 순익 2·3위로… 메리츠·NH 넘어서나?  2024.07.16 16:35   \n",
       "...         ...                                         ...               ...   \n",
       "1237297  462870            IPO 앞둔 시프트업, `희망퇴직·성추문 의혹` 잇단 악재  2023.07.24 16:55   \n",
       "1237298  462870  [단독] '성추문·폭언 의혹' 시프트업 투자사 대표 \"책임지고 퇴사하겠...  2023.07.24 12:53   \n",
       "1237299  462870               시프트업 '데스티니차일드', 9월 21일 서비스 종료  2023.07.20 17:07   \n",
       "1237300  462870              시프트업 '데스티니 차일드', 9월 21일 서비스 종료  2023.07.20 16:47   \n",
       "1237301  462870               시프트업 '데스티니 차일드', 9월21일 서비스 종료  2023.07.20 16:22   \n",
       "\n",
       "        info_provider                                          full_link  \n",
       "0                이데일리  https://n.news.naver.com/mnews/article/018/000...  \n",
       "1                한국경제  https://n.news.naver.com/mnews/article/015/000...  \n",
       "2                 뉴스1  https://n.news.naver.com/mnews/article/421/000...  \n",
       "3               아시아경제  https://n.news.naver.com/mnews/article/277/000...  \n",
       "4                 머니S  https://n.news.naver.com/mnews/article/417/000...  \n",
       "...               ...                                                ...  \n",
       "1237297        디지털타임스  https://n.news.naver.com/mnews/article/029/000...  \n",
       "1237298        아이뉴스24  https://n.news.naver.com/mnews/article/031/000...  \n",
       "1237299        지디넷코리아  https://n.news.naver.com/mnews/article/092/000...  \n",
       "1237300          전자신문  https://n.news.naver.com/mnews/article/030/000...  \n",
       "1237301       데일리e스포츠  https://n.news.naver.com/mnews/article/347/000...  \n",
       "\n",
       "[1237302 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_df = pd.DataFrame(all_news, columns=['sid', 'title', 'date', 'info_provider', 'full_link'])\n",
    "all_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top100_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_news_df.to_pickle(DATA_PATH / 'all_news_df_top100.pkl') # 체크포인트. 200MB 넘음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_df = pd.read_pickle(DATA_PATH / 'all_news_df_top100.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 본문 크롤링\n",
    "\n",
    "제목만 크롤링했는데 200MB 넘는다. 본문 크롤링 시 \n",
    "- 일단 request가 확 늘어남. 한 페이지당 뉴스 10개니 10배. \n",
    "- 용량도 확 늘어남. 기사 본문이 제목보다 100배 더 길다고 치면 20GB\n",
    "    - 메모리 관리 위해 분할 저장하는 전략이 필요할 수 있음. every 100개마다 pickle로 떨구고 다시 가자. \n",
    "        - 일단 이렇게 해놓겠음. \n",
    "    - 그러더라도 분석을 위해선 결국 다 올려야 할 수 있음. 이 경우 다음 전략을 고려\n",
    "        - 전략 1: \n",
    "            - 100개씩 나눠놓은 dataframe을 vector DB화\n",
    "            - vector input으로 LLM 돌릴 수 있는 방법이 있다면 이것이 최선일 수 있음. \n",
    "            - 그러나, 우리의 UX대로 나중에 기사 원문을 보기 위해선 역 reference가 가능하도록 구현해야 함. \n",
    "                - 또는, 굳이 그럴 것 없이 href 남겨져 있으니까 기사 원문 보고 싶으면 클릭해서 네이버 뉴스로 보내는 방법도 있음. \n",
    "                - 사실 저작권을 고려하면 이렇게 구현하는 것이 맞음. \n",
    "        - 전략 2:\n",
    "            - 100개 나눠놓은 dataframe을 LLM 돌려 graph RAG 연결관계 만들어내고, \n",
    "            - 각 df에서 본문 drop시키고\n",
    "            - 모든 df를 concat. --> 기사 timestamp, 제목, 연결관계만 남음. \n",
    "            - Hyper CLOVA vector DB 연동 어떻게 되는지 잘 모르므로 이게 나을 수 있음. \n",
    "            - 역 reference는 위와 같은 방법으로 네이버 뉴스 href 제공. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Split the DataFrame into chunks of N rows each\n",
    "N = 100000\n",
    "chunks = [all_news_df.iloc[i:i + N ] for i in range(0, len(all_news_df), N)]\n",
    "\n",
    "# Function to scrape news article body from the URL\n",
    "def get_article_body(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_body = soup.find('article', {'id': 'dic_area'}).get_text(strip=True)\n",
    "        return article_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_link\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_article_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mto_pickle(DATA_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_full_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping and saving completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m, in \u001b[0;36mget_article_body\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_article_body\u001b[39m(url):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m         article_body \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdic_area\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chlje\\VSCodeProjects\\miraeLLMai2024\\.venv\\Lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each chunk\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {idx}...')\n",
    "    chunk['article'] = chunk['full_link'].apply(get_article_body)\n",
    "    chunk.to_pickle(DATA_PATH / f\"news_full_{str(idx).zfill(2)}.pkl\")\n",
    "\n",
    "print(\"Scraping and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
