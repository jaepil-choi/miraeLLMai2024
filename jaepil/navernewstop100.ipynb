{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 종목에 대한 Naver 뉴스 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd()\n",
    "WORKSPACE_PATH = CWD.parent\n",
    "COMMON_PATH = WORKSPACE_PATH / 'common'\n",
    "DATA_PATH = WORKSPACE_PATH / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기 (Top 100 returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df = pd.read_pickle(DATA_PATH / 'returns_df_top100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020.01.02', '2024.07.12')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE = returns_df.index[0].strftime('%Y.%m.%d')\n",
    "END_DATE = returns_df.index[-1].strftime('%Y.%m.%d')\n",
    "\n",
    "START_DATE, END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_tickers = returns_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(DATA_PATH / 'top100_tickers.pkl', 'wb') as f:\n",
    "#     pickle.dump(top100_tickers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 종목뉴스 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 제목, 링크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(sid, page):\n",
    "    sid = str(sid).zfill(6)\n",
    "    url = f'https://finance.naver.com/item/news_news.naver?code={sid}&page={page}'\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    news_list = []\n",
    "    table = soup.find('table', {'class': 'type5'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        # Identify main article rows\n",
    "        if row.get('class') is None or 'relation_tit' in row.get('class', []):\n",
    "            cols = row.find_all('td')\n",
    "            if cols and cols[0].find('a', {'class': 'tit'}):\n",
    "                title_tag = cols[0].find('a', {'class': 'tit'})\n",
    "                if title_tag:\n",
    "                    title = title_tag.text.strip()\n",
    "                    href = title_tag['href']\n",
    "                    if 'news_read' in href:\n",
    "                        params = href.split('?')[1]\n",
    "                        params_dict = dict(param.split('=') for param in params.split('&'))\n",
    "                        office_id = params_dict.get('office_id')\n",
    "                        article_id = params_dict.get('article_id')\n",
    "                        full_link = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'\n",
    "                        info_provider = cols[1].text.strip()\n",
    "                        date = cols[2].text.strip()\n",
    "                        news_list.append([sid, title, date, info_provider, full_link])\n",
    "        # Identify child article rows\n",
    "        elif 'relation_lst' in row.get('class', []):\n",
    "            sub_table = row.find('table', {'class': 'type5'})\n",
    "            sub_rows = sub_table.find_all('tr')\n",
    "            for sub_row in sub_rows:\n",
    "                sub_cols = sub_row.find_all('td')\n",
    "                if sub_cols and sub_cols[0].find('a', {'class': 'tit'}):\n",
    "                    title_tag = sub_cols[0].find('a', {'class': 'tit'})\n",
    "                    if title_tag:\n",
    "                        title = title_tag.text.strip()\n",
    "                        href = title_tag['href']\n",
    "                        if 'news_read' in href:\n",
    "                            params = href.split('?')[1]\n",
    "                            params_dict = dict(param.split('=') for param in params.split('&'))\n",
    "                            office_id = params_dict.get('office_id')\n",
    "                            article_id = params_dict.get('article_id')\n",
    "                            full_link = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'\n",
    "                            info_provider = sub_cols[1].text.strip()\n",
    "                            date = sub_cols[2].text.strip()\n",
    "                            news_list.append([sid, title, date, info_provider, full_link])\n",
    "    return news_list\n",
    "\n",
    "# Function to scrape multiple pages until a specific date\n",
    "def scrape_until_date(sid, end_date):\n",
    "    page = 1\n",
    "    all_news = []\n",
    "    last_page_content = None\n",
    "    \n",
    "    while True:\n",
    "        news_list = scrape_page(sid, page)\n",
    "        if not news_list:\n",
    "            break\n",
    "        \n",
    "        current_page_content = str(news_list)\n",
    "        if current_page_content == last_page_content:\n",
    "            break\n",
    "\n",
    "        for news in news_list:\n",
    "            news_date = news[2]\n",
    "            if news_date < end_date:\n",
    "                return all_news\n",
    "            \n",
    "            all_news.append(news)\n",
    "        \n",
    "        last_page_content = current_page_content\n",
    "        page += 1\n",
    "\n",
    "    return all_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sid = '007110'  # Example stock code\n",
    "end_date = '2024.07.14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020.01.02'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping #0: 000060...\n",
      "Time taken for scraping #0: 6.68 seconds\n",
      "Scraping #1: 000080...\n",
      "Time taken for scraping #1: 55.85 seconds\n",
      "Scraping #2: 000100...\n",
      "Time taken for scraping #2: 37.04 seconds\n",
      "Scraping #3: 000120...\n",
      "Time taken for scraping #3: 51.01 seconds\n",
      "Scraping #4: 000150...\n",
      "Time taken for scraping #4: 33.23 seconds\n",
      "Scraping #5: 000210...\n",
      "Time taken for scraping #5: 4.74 seconds\n",
      "Scraping #6: 000250...\n",
      "Time taken for scraping #6: 15.00 seconds\n",
      "Scraping #7: 000270...\n",
      "Time taken for scraping #7: 62.99 seconds\n",
      "Scraping #8: 000660...\n",
      "Time taken for scraping #8: 66.12 seconds\n",
      "Scraping #9: 000720...\n",
      "Time taken for scraping #9: 58.14 seconds\n",
      "Scraping #10: 000810...\n",
      "Time taken for scraping #10: 52.75 seconds\n",
      "Scraping #11: 000990...\n",
      "Time taken for scraping #11: 10.11 seconds\n",
      "Scraping #12: 001040...\n",
      "Time taken for scraping #12: 51.20 seconds\n",
      "Scraping #13: 001440...\n",
      "Time taken for scraping #13: 17.15 seconds\n",
      "Scraping #14: 001450...\n",
      "Time taken for scraping #14: 51.80 seconds\n",
      "Scraping #15: 001570...\n",
      "Time taken for scraping #15: 10.65 seconds\n",
      "Scraping #16: 002380...\n",
      "Time taken for scraping #16: 11.78 seconds\n",
      "Scraping #17: 002790...\n",
      "Time taken for scraping #17: 54.13 seconds\n",
      "Scraping #18: 003000...\n",
      "Time taken for scraping #18: 6.00 seconds\n",
      "Scraping #19: 003090...\n",
      "Time taken for scraping #19: 0.52 seconds\n",
      "Scraping #20: 003230...\n",
      "Time taken for scraping #20: 40.13 seconds\n",
      "Scraping #21: 003410...\n",
      "Time taken for scraping #21: 0.16 seconds\n",
      "Scraping #22: 003490...\n",
      "Time taken for scraping #22: 59.85 seconds\n",
      "Scraping #23: 003550...\n",
      "Time taken for scraping #23: 61.69 seconds\n",
      "Scraping #24: 003670...\n",
      "Time taken for scraping #24: 56.64 seconds\n",
      "Scraping #25: 004020...\n",
      "Time taken for scraping #25: 45.76 seconds\n",
      "Scraping #26: 004170...\n",
      "Time taken for scraping #26: 66.06 seconds\n",
      "Scraping #27: 004990...\n",
      "Time taken for scraping #27: 29.06 seconds\n",
      "Scraping #28: 005070...\n",
      "Time taken for scraping #28: 5.73 seconds\n",
      "Scraping #29: 005380...\n",
      "Time taken for scraping #29: 73.38 seconds\n",
      "Scraping #30: 005420...\n",
      "Time taken for scraping #30: 2.30 seconds\n",
      "Scraping #31: 005490...\n",
      "Time taken for scraping #31: 63.95 seconds\n",
      "Scraping #32: 005830...\n",
      "Time taken for scraping #32: 45.93 seconds\n",
      "Scraping #33: 005930...\n",
      "Time taken for scraping #33: 76.00 seconds\n",
      "Scraping #34: 005940...\n",
      "Time taken for scraping #34: 58.36 seconds\n",
      "Scraping #35: 006260...\n",
      "Time taken for scraping #35: 29.19 seconds\n",
      "Scraping #36: 006280...\n",
      "Time taken for scraping #36: 23.22 seconds\n",
      "Scraping #37: 006360...\n",
      "Time taken for scraping #37: 62.94 seconds\n",
      "Scraping #38: 006400...\n",
      "Time taken for scraping #38: 63.12 seconds\n",
      "Scraping #39: 006800...\n",
      "Time taken for scraping #39: 57.90 seconds\n",
      "Scraping #40: 007070...\n",
      "Time taken for scraping #40: 57.48 seconds\n",
      "Scraping #41: 008560...\n",
      "Time taken for scraping #41: 28.78 seconds\n",
      "Scraping #42: 008770...\n",
      "Time taken for scraping #42: 64.10 seconds\n",
      "Scraping #43: 008930...\n",
      "Time taken for scraping #43: 44.19 seconds\n",
      "Scraping #44: 009150...\n",
      "Time taken for scraping #44: 57.25 seconds\n",
      "Scraping #45: 009240...\n",
      "Time taken for scraping #45: 13.26 seconds\n",
      "Scraping #46: 009540...\n",
      "Time taken for scraping #46: 58.40 seconds\n",
      "Scraping #47: 009830...\n",
      "Time taken for scraping #47: 52.06 seconds\n",
      "Scraping #48: 010060...\n",
      "Time taken for scraping #48: 29.58 seconds\n",
      "Scraping #49: 010120...\n",
      "Time taken for scraping #49: 48.00 seconds\n",
      "Scraping #50: 010130...\n",
      "Time taken for scraping #50: 39.39 seconds\n",
      "Scraping #51: 010140...\n",
      "Time taken for scraping #51: 53.17 seconds\n",
      "Scraping #52: 010620...\n",
      "Time taken for scraping #52: 24.44 seconds\n",
      "Scraping #53: 010950...\n",
      "Time taken for scraping #53: 47.74 seconds\n",
      "Scraping #54: 011070...\n",
      "Time taken for scraping #54: 60.35 seconds\n",
      "Scraping #55: 011170...\n",
      "Time taken for scraping #55: 80.20 seconds\n",
      "Scraping #56: 011200...\n",
      "Time taken for scraping #56: 26.29 seconds\n",
      "Scraping #57: 011780...\n",
      "Time taken for scraping #57: 26.27 seconds\n",
      "Scraping #58: 011790...\n",
      "Time taken for scraping #58: 16.24 seconds\n",
      "Scraping #59: 012330...\n",
      "Time taken for scraping #59: 58.48 seconds\n",
      "Scraping #60: 012450...\n",
      "Time taken for scraping #60: 57.87 seconds\n",
      "Scraping #61: 012510...\n",
      "Time taken for scraping #61: 12.76 seconds\n",
      "Scraping #62: 012750...\n",
      "Time taken for scraping #62: 8.41 seconds\n",
      "Scraping #63: 014680...\n",
      "Time taken for scraping #63: 2.60 seconds\n",
      "Scraping #64: 015760...\n",
      "Time taken for scraping #64: 66.95 seconds\n",
      "Scraping #65: 016360...\n",
      "Time taken for scraping #65: 64.02 seconds\n",
      "Scraping #66: 017670...\n",
      "Time taken for scraping #66: 76.82 seconds\n",
      "Scraping #67: 018260...\n",
      "Time taken for scraping #67: 61.18 seconds\n",
      "Scraping #68: 018880...\n",
      "Time taken for scraping #68: 7.41 seconds\n",
      "Scraping #69: 019170...\n",
      "Time taken for scraping #69: 2.86 seconds\n",
      "Scraping #70: 020150...\n",
      "Time taken for scraping #70: 17.25 seconds\n",
      "Scraping #71: 021240...\n",
      "Time taken for scraping #71: 23.92 seconds\n",
      "Scraping #72: 022100...\n",
      "Time taken for scraping #72: 58.13 seconds\n",
      "Scraping #73: 023530...\n",
      "Time taken for scraping #73: 52.01 seconds\n",
      "Scraping #74: 024110...\n",
      "Time taken for scraping #74: 62.67 seconds\n",
      "Scraping #75: 026960...\n",
      "Time taken for scraping #75: 5.49 seconds\n",
      "Scraping #76: 028050...\n",
      "Time taken for scraping #76: 22.76 seconds\n",
      "Scraping #77: 028260...\n",
      "Time taken for scraping #77: 62.91 seconds\n",
      "Scraping #78: 028300...\n",
      "Time taken for scraping #78: 38.99 seconds\n",
      "Scraping #79: 028670...\n",
      "Time taken for scraping #79: 23.64 seconds\n",
      "Scraping #80: 029780...\n",
      "Time taken for scraping #80: 36.51 seconds\n",
      "Scraping #81: 030200...\n",
      "Time taken for scraping #81: 73.71 seconds\n",
      "Scraping #82: 032500...\n",
      "Time taken for scraping #82: 0.58 seconds\n",
      "Scraping #83: 032640...\n",
      "Time taken for scraping #83: 70.87 seconds\n",
      "Scraping #84: 032830...\n",
      "Time taken for scraping #84: 59.66 seconds\n",
      "Scraping #85: 033780...\n",
      "Time taken for scraping #85: 43.97 seconds\n",
      "Scraping #86: 034020...\n",
      "Time taken for scraping #86: 63.45 seconds\n",
      "Scraping #87: 034220...\n",
      "Time taken for scraping #87: 64.98 seconds\n",
      "Scraping #88: 034730...\n",
      "Time taken for scraping #88: 76.67 seconds\n",
      "Scraping #89: 035250...\n",
      "Time taken for scraping #89: 15.42 seconds\n",
      "Scraping #90: 035420...\n",
      "Time taken for scraping #90: 77.01 seconds\n",
      "Scraping #91: 035720...\n",
      "Time taken for scraping #91: 73.68 seconds\n",
      "Scraping #92: 035760...\n",
      "Time taken for scraping #92: 34.48 seconds\n",
      "Scraping #93: 035900...\n",
      "Time taken for scraping #93: 52.15 seconds\n",
      "Scraping #94: 036460...\n",
      "Time taken for scraping #94: 53.11 seconds\n",
      "Scraping #95: 036490...\n",
      "Time taken for scraping #95: 0.08 seconds\n",
      "Scraping #96: 036570...\n",
      "Time taken for scraping #96: 80.36 seconds\n",
      "Scraping #97: 039490...\n",
      "Time taken for scraping #97: 59.31 seconds\n",
      "Scraping #98: 041510...\n",
      "Time taken for scraping #98: 58.40 seconds\n",
      "Scraping #99: 041960...\n",
      "Time taken for scraping #99: 0.54 seconds\n",
      "Scraping #100: 042660...\n",
      "Time taken for scraping #100: 64.65 seconds\n",
      "Scraping #101: 042670...\n",
      "Time taken for scraping #101: 22.88 seconds\n",
      "Scraping #102: 042700...\n",
      "Time taken for scraping #102: 44.42 seconds\n",
      "Scraping #103: 047040...\n",
      "Time taken for scraping #103: 59.58 seconds\n",
      "Scraping #104: 047050...\n",
      "Time taken for scraping #104: 57.75 seconds\n",
      "Scraping #105: 047810...\n",
      "Time taken for scraping #105: 52.73 seconds\n",
      "Scraping #106: 051900...\n",
      "Time taken for scraping #106: 60.26 seconds\n",
      "Scraping #107: 051910...\n",
      "Time taken for scraping #107: 60.98 seconds\n",
      "Scraping #108: 052690...\n",
      "Time taken for scraping #108: 10.42 seconds\n",
      "Scraping #109: 055550...\n",
      "Time taken for scraping #109: 70.21 seconds\n",
      "Scraping #110: 058470...\n",
      "Time taken for scraping #110: 40.72 seconds\n",
      "Scraping #111: 064350...\n",
      "Time taken for scraping #111: 40.05 seconds\n",
      "Scraping #112: 065350...\n",
      "Time taken for scraping #112: 21.56 seconds\n",
      "Scraping #113: 066570...\n",
      "Time taken for scraping #113: 76.74 seconds\n",
      "Scraping #114: 066970...\n",
      "Time taken for scraping #114: 36.87 seconds\n",
      "Scraping #115: 068270...\n",
      "Time taken for scraping #115: 57.99 seconds\n",
      "Scraping #116: 068760...\n",
      "Time taken for scraping #116: 53.39 seconds\n",
      "Scraping #117: 069620...\n",
      "Time taken for scraping #117: 37.36 seconds\n",
      "Scraping #118: 071050...\n",
      "Time taken for scraping #118: 55.34 seconds\n",
      "Scraping #119: 078930...\n",
      "Time taken for scraping #119: 39.01 seconds\n",
      "Scraping #120: 079440...\n",
      "Time taken for scraping #120: 0.06 seconds\n",
      "Scraping #121: 079550...\n",
      "Time taken for scraping #121: 42.80 seconds\n",
      "Scraping #122: 081660...\n",
      "Time taken for scraping #122: 7.91 seconds\n",
      "Scraping #123: 084990...\n",
      "Time taken for scraping #123: 2.58 seconds\n",
      "Scraping #124: 086280...\n",
      "Time taken for scraping #124: 23.58 seconds\n",
      "Scraping #125: 086520...\n",
      "Time taken for scraping #125: 58.48 seconds\n",
      "Scraping #126: 086790...\n",
      "Time taken for scraping #126: 67.50 seconds\n",
      "Scraping #127: 086900...\n",
      "Time taken for scraping #127: 13.63 seconds\n",
      "Scraping #128: 088350...\n",
      "Time taken for scraping #128: 59.37 seconds\n",
      "Scraping #129: 090430...\n",
      "Time taken for scraping #129: 56.66 seconds\n",
      "Scraping #130: 091990...\n",
      "Time taken for scraping #130: 52.61 seconds\n",
      "Scraping #131: 095700...\n",
      "Time taken for scraping #131: 2.62 seconds\n",
      "Scraping #132: 096530...\n",
      "Time taken for scraping #132: 3.39 seconds\n",
      "Scraping #133: 096770...\n",
      "Time taken for scraping #133: 64.35 seconds\n",
      "Scraping #134: 097950...\n",
      "Time taken for scraping #134: 63.03 seconds\n",
      "Scraping #135: 105560...\n",
      "Time taken for scraping #135: 62.56 seconds\n",
      "Scraping #136: 112040...\n",
      "Time taken for scraping #136: 77.64 seconds\n",
      "Scraping #137: 112610...\n",
      "Time taken for scraping #137: 5.58 seconds\n",
      "Scraping #138: 128940...\n",
      "Time taken for scraping #138: 60.54 seconds\n",
      "Scraping #139: 137310...\n",
      "Time taken for scraping #139: 4.30 seconds\n",
      "Scraping #140: 138040...\n",
      "Time taken for scraping #140: 20.72 seconds\n",
      "Scraping #141: 139480...\n",
      "Time taken for scraping #141: 67.18 seconds\n",
      "Scraping #142: 145020...\n",
      "Time taken for scraping #142: 10.93 seconds\n",
      "Scraping #143: 161390...\n",
      "Time taken for scraping #143: 48.05 seconds\n",
      "Scraping #144: 180640...\n",
      "Time taken for scraping #144: 9.62 seconds\n",
      "Scraping #145: 196170...\n",
      "Time taken for scraping #145: 51.63 seconds\n",
      "Scraping #146: 204320...\n",
      "Time taken for scraping #146: 9.29 seconds\n",
      "Scraping #147: 207940...\n",
      "Time taken for scraping #147: 58.77 seconds\n",
      "Scraping #148: 235980...\n",
      "Time taken for scraping #148: 2.96 seconds\n",
      "Scraping #149: 241560...\n",
      "Time taken for scraping #149: 18.91 seconds\n",
      "Scraping #150: 247540...\n",
      "Time taken for scraping #150: 54.91 seconds\n",
      "Scraping #151: 251270...\n",
      "Time taken for scraping #151: 58.39 seconds\n",
      "Scraping #152: 253450...\n",
      "Time taken for scraping #152: 9.78 seconds\n",
      "Scraping #153: 259960...\n",
      "Time taken for scraping #153: 58.45 seconds\n",
      "Scraping #154: 263750...\n",
      "Time taken for scraping #154: 43.62 seconds\n",
      "Scraping #155: 267250...\n",
      "Time taken for scraping #155: 62.38 seconds\n",
      "Scraping #156: 267260...\n",
      "Time taken for scraping #156: 26.47 seconds\n",
      "Scraping #157: 268600...\n",
      "Time taken for scraping #157: 2.00 seconds\n",
      "Scraping #158: 271560...\n",
      "Time taken for scraping #158: 22.23 seconds\n",
      "Scraping #159: 272210...\n",
      "Time taken for scraping #159: 53.48 seconds\n",
      "Scraping #160: 277810...\n",
      "Time taken for scraping #160: 52.27 seconds\n",
      "Scraping #161: 278280...\n",
      "Time taken for scraping #161: 0.83 seconds\n",
      "Scraping #162: 282330...\n",
      "Time taken for scraping #162: 53.18 seconds\n",
      "Scraping #163: 285130...\n",
      "Time taken for scraping #163: 2.80 seconds\n",
      "Scraping #164: 293490...\n",
      "Time taken for scraping #164: 59.83 seconds\n",
      "Scraping #165: 298020...\n",
      "Time taken for scraping #165: 3.03 seconds\n",
      "Scraping #166: 298040...\n",
      "Time taken for scraping #166: 5.27 seconds\n",
      "Scraping #167: 298050...\n",
      "Time taken for scraping #167: 2.85 seconds\n",
      "Scraping #168: 302440...\n",
      "Time taken for scraping #168: 26.56 seconds\n",
      "Scraping #169: 307950...\n",
      "Time taken for scraping #169: 19.30 seconds\n",
      "Scraping #170: 316140...\n",
      "Time taken for scraping #170: 61.43 seconds\n",
      "Scraping #171: 323410...\n",
      "Time taken for scraping #171: 61.66 seconds\n",
      "Scraping #172: 323990...\n",
      "Time taken for scraping #172: 2.90 seconds\n",
      "Scraping #173: 326030...\n",
      "Time taken for scraping #173: 25.12 seconds\n",
      "Scraping #174: 328130...\n",
      "Time taken for scraping #174: 16.19 seconds\n",
      "Scraping #175: 329180...\n",
      "Time taken for scraping #175: 63.96 seconds\n",
      "Scraping #176: 336260...\n",
      "Time taken for scraping #176: 2.98 seconds\n",
      "Scraping #177: 348370...\n",
      "Time taken for scraping #177: 14.16 seconds\n",
      "Scraping #178: 352820...\n",
      "Time taken for scraping #178: 45.42 seconds\n",
      "Scraping #179: 361610...\n",
      "Time taken for scraping #179: 15.71 seconds\n",
      "Scraping #180: 373220...\n",
      "Time taken for scraping #180: 66.25 seconds\n",
      "Scraping #181: 377300...\n",
      "Time taken for scraping #181: 65.65 seconds\n",
      "Scraping #182: 383220...\n",
      "Time taken for scraping #182: 1.50 seconds\n",
      "Scraping #183: 402340...\n",
      "Time taken for scraping #183: 32.70 seconds\n",
      "Scraping #184: 403870...\n",
      "Time taken for scraping #184: 17.29 seconds\n",
      "Scraping #185: 417200...\n",
      "Time taken for scraping #185: 17.05 seconds\n",
      "Scraping #186: 443060...\n",
      "Time taken for scraping #186: 25.76 seconds\n",
      "Scraping #187: 450080...\n",
      "Time taken for scraping #187: 41.72 seconds\n",
      "Scraping #188: 454910...\n",
      "Time taken for scraping #188: 54.88 seconds\n",
      "Scraping #189: 462870...\n",
      "Time taken for scraping #189: 4.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "all_news = []\n",
    "\n",
    "for i, sid in enumerate(top100_tickers):\n",
    "    sid = sid[1:]\n",
    "    print(f'Scraping #{i}: {sid}...')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    all_news += scrape_until_date(sid, START_DATE)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_taken = end_time - start_time\n",
    "    print(f'Time taken for scraping #{i}: {time_taken:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>info_provider</th>\n",
       "      <th>full_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000060</td>\n",
       "      <td>'여의도 금융중심 계획' 결정고시 눈앞…시행사들 기다림 끝나간다</td>\n",
       "      <td>2024.07.18 21:03</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/018/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000060</td>\n",
       "      <td>힘내요, 한 발 한 발…든든한 금융지주가 사다리를 놓아줍니다</td>\n",
       "      <td>2024.07.18 16:26</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000060</td>\n",
       "      <td>\"휴대폰 파손·항공 지연 대비\"…네이버페이 여행보험 플랜 비교</td>\n",
       "      <td>2024.07.18 10:16</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000060</td>\n",
       "      <td>보험사 2분기 실적, 생보 웃고 손보 운다…제3보험·車보험 변수</td>\n",
       "      <td>2024.07.17 08:44</td>\n",
       "      <td>아시아경제</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/277/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000060</td>\n",
       "      <td>진격의 삼성·키움증권, 순익 2·3위로… 메리츠·NH 넘어서나?</td>\n",
       "      <td>2024.07.16 16:35</td>\n",
       "      <td>머니S</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/417/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237297</th>\n",
       "      <td>462870</td>\n",
       "      <td>IPO 앞둔 시프트업, `희망퇴직·성추문 의혹` 잇단 악재</td>\n",
       "      <td>2023.07.24 16:55</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/029/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237298</th>\n",
       "      <td>462870</td>\n",
       "      <td>[단독] '성추문·폭언 의혹' 시프트업 투자사 대표 \"책임지고 퇴사하겠...</td>\n",
       "      <td>2023.07.24 12:53</td>\n",
       "      <td>아이뉴스24</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/031/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237299</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니차일드', 9월 21일 서비스 종료</td>\n",
       "      <td>2023.07.20 17:07</td>\n",
       "      <td>지디넷코리아</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/092/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237300</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니 차일드', 9월 21일 서비스 종료</td>\n",
       "      <td>2023.07.20 16:47</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/030/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237301</th>\n",
       "      <td>462870</td>\n",
       "      <td>시프트업 '데스티니 차일드', 9월21일 서비스 종료</td>\n",
       "      <td>2023.07.20 16:22</td>\n",
       "      <td>데일리e스포츠</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/347/000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1237302 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid                                       title              date  \\\n",
       "0        000060         '여의도 금융중심 계획' 결정고시 눈앞…시행사들 기다림 끝나간다  2024.07.18 21:03   \n",
       "1        000060           힘내요, 한 발 한 발…든든한 금융지주가 사다리를 놓아줍니다  2024.07.18 16:26   \n",
       "2        000060          \"휴대폰 파손·항공 지연 대비\"…네이버페이 여행보험 플랜 비교  2024.07.18 10:16   \n",
       "3        000060         보험사 2분기 실적, 생보 웃고 손보 운다…제3보험·車보험 변수  2024.07.17 08:44   \n",
       "4        000060         진격의 삼성·키움증권, 순익 2·3위로… 메리츠·NH 넘어서나?  2024.07.16 16:35   \n",
       "...         ...                                         ...               ...   \n",
       "1237297  462870            IPO 앞둔 시프트업, `희망퇴직·성추문 의혹` 잇단 악재  2023.07.24 16:55   \n",
       "1237298  462870  [단독] '성추문·폭언 의혹' 시프트업 투자사 대표 \"책임지고 퇴사하겠...  2023.07.24 12:53   \n",
       "1237299  462870               시프트업 '데스티니차일드', 9월 21일 서비스 종료  2023.07.20 17:07   \n",
       "1237300  462870              시프트업 '데스티니 차일드', 9월 21일 서비스 종료  2023.07.20 16:47   \n",
       "1237301  462870               시프트업 '데스티니 차일드', 9월21일 서비스 종료  2023.07.20 16:22   \n",
       "\n",
       "        info_provider                                          full_link  \n",
       "0                이데일리  https://n.news.naver.com/mnews/article/018/000...  \n",
       "1                한국경제  https://n.news.naver.com/mnews/article/015/000...  \n",
       "2                 뉴스1  https://n.news.naver.com/mnews/article/421/000...  \n",
       "3               아시아경제  https://n.news.naver.com/mnews/article/277/000...  \n",
       "4                 머니S  https://n.news.naver.com/mnews/article/417/000...  \n",
       "...               ...                                                ...  \n",
       "1237297        디지털타임스  https://n.news.naver.com/mnews/article/029/000...  \n",
       "1237298        아이뉴스24  https://n.news.naver.com/mnews/article/031/000...  \n",
       "1237299        지디넷코리아  https://n.news.naver.com/mnews/article/092/000...  \n",
       "1237300          전자신문  https://n.news.naver.com/mnews/article/030/000...  \n",
       "1237301       데일리e스포츠  https://n.news.naver.com/mnews/article/347/000...  \n",
       "\n",
       "[1237302 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_df = pd.DataFrame(all_news, columns=['sid', 'title', 'date', 'info_provider', 'full_link'])\n",
    "all_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top100_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_df.to_pickle(DATA_PATH / 'all_news_df_top100.pkl') # 체크포인트. 200MB 넘음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_df = pd.read_pickle(DATA_PATH / 'all_news_df_top100.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 본문 크롤링\n",
    "\n",
    "제목만 크롤링했는데 200MB 넘는다. 본문 크롤링 시 \n",
    "- 일단 request가 확 늘어남. 한 페이지당 뉴스 10개니 10배. \n",
    "- 용량도 확 늘어남. 기사 본문이 제목보다 100배 더 길다고 치면 20GB\n",
    "    - 메모리 관리 위해 분할 저장하는 전략이 필요할 수 있음. every 100개마다 pickle로 떨구고 다시 가자. \n",
    "        - 일단 이렇게 해놓겠음. \n",
    "    - 그러더라도 분석을 위해선 결국 다 올려야 할 수 있음. 이 경우 다음 전략을 고려\n",
    "        - 전략 1: \n",
    "            - 100개씩 나눠놓은 dataframe을 vector DB화\n",
    "            - vector input으로 LLM 돌릴 수 있는 방법이 있다면 이것이 최선일 수 있음. \n",
    "            - 그러나, 우리의 UX대로 나중에 기사 원문을 보기 위해선 역 reference가 가능하도록 구현해야 함. \n",
    "                - 또는, 굳이 그럴 것 없이 href 남겨져 있으니까 기사 원문 보고 싶으면 클릭해서 네이버 뉴스로 보내는 방법도 있음. \n",
    "                - 사실 저작권을 고려하면 이렇게 구현하는 것이 맞음. \n",
    "        - 전략 2:\n",
    "            - 100개 나눠놓은 dataframe을 LLM 돌려 graph RAG 연결관계 만들어내고, \n",
    "            - 각 df에서 본문 drop시키고\n",
    "            - 모든 df를 concat. --> 기사 timestamp, 제목, 연결관계만 남음. \n",
    "            - Hyper CLOVA vector DB 연동 어떻게 되는지 잘 모르므로 이게 나을 수 있음. \n",
    "            - 역 reference는 위와 같은 방법으로 네이버 뉴스 href 제공. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
